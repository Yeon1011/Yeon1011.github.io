[ { "title": "[Grafana] 설치 및 구동 테스트", "url": "/posts/aa-grafana-test/", "categories": "BigData, Grafana", "tags": "Grafana, Test", "date": "2022-01-06 00:00:00 +0900", "snippet": "Grafana 설치 및 테스트환경 구성 Tools Version VirtualBox 6.1 CentOS 7 Elasticsearch 7.16.2 Grafana 8.3.3 사용포트 정리 Program Port Number Elasticsearch 9200 Grafana 3000 설치[yeon@yeon-host grafana]$ pwd/home/yeon/lib/grafana[yeon@yeon-host grafana] wget https://dl.grafana.com/enterprise/release/grafana-enterprise-8.3.3.linux-amd64.tar.gz[yeon@yeon-host grafana] tar -zxvf grafana-enterprise-8.3.3.linux-amd64.tar.gz구동구동방법[yeon@yeon-host bin]$ pwd/home/yeon/dev/grafana/bin[yeon@yeon-host bin]$ grafana-server구동후 접속 테스트[yeon@yeon-host elasticsearch]$ curl -put &#39;http://192.168.25.2:9200/&#39;Enter host password for user &#39;t&#39;:{ &quot;name&quot; : &quot;yeon-host&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;LAxAWhHnQe6SjY6YxUbzUw&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;7.16.2&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;2b937c44140b6559905130a8650c64dbd0879cfb&quot;, &quot;build_date&quot; : &quot;2021-12-18T19:42:46.604893745Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;8.10.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} 방화벽 3000 포트 허용[root@yeon-host ~]# firewall-cmd --permanent --zone=public --add-port=3000/tcpsuccess[root@yeon-host ~]# firewall-cmd --reloadsuccess 다른 IP에서 접속했을 경우다른IP 브라우저에서 url을 호출했을 경우server로 구동 적용해보진 않음.Start the server with systemdTo start the service and verify that the service has started:sudo systemctl daemon-reloadsudo systemctl start grafana-serversudo systemctl status grafana-server# Configure the Grafana server to start at boot:sudo systemctl enable grafana-server" }, { "title": "[Elasticsearch] 구동 테스트 및 오류", "url": "/posts/aa-elasticsearch-test/", "categories": "BigData, Elasticsearch", "tags": "Elasticsearch, Test", "date": "2022-01-06 00:00:00 +0900", "snippet": "Elasticsearch 설치 및 테스트환경 구성 Tools Version VirtualBox 6.1 CentOS 7 Elasticsearch 7.16.2 Grafana 8.3.3 사용포트 정리 Program Port Number Elasticsearch 9200 Grafana 3000 설치구동방법[yeon@yeon-host elasticsearch]$ pwd/home/yeon/dev/elasticsearch[yeon@yeon-host elasticsearch]$ bin/elasticsearch구동후 접속 테스트[yeon@yeon-host elasticsearch]$ curl -put &#39;http://192.168.25.2:9200/&#39;Enter host password for user &#39;t&#39;:{ &quot;name&quot; : &quot;yeon-host&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;LAxAWhHnQe6SjY6YxUbzUw&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;7.16.2&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;2b937c44140b6559905130a8650c64dbd0879cfb&quot;, &quot;build_date&quot; : &quot;2021-12-18T19:42:46.604893745Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;8.10.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} 방화벽 9200 포트 허용[root@yeon-host ~]# firewall-cmd --permanent --zone=public --add-port=9200/tcpsuccess[root@yeon-host ~]# firewall-cmd --reloadsuccess 다른 IP에서 접속했을 경우다른IP 브라우저에서 url을 호출했을 경우구동 오류(1)[2022-01-06T00:15:11,369][INFO ][o.e.b.BootstrapChecks ] [yeon-host] bound or publishing to a non-loopback address, enforcing bootstrap checksERROR: [2] bootstrap checks failed. You must address the points described in the following [2] lines before starting Elasticsearch.bootstrap check failure [1] of [2]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]bootstrap check failure [2] of [2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]ERROR: Elasticsearch did not exit normally - check the logs at /home/yeon/dev/elasticsearch/logs/elasticsearch.log[2022-01-06T00:15:11,474][INFO ][o.e.n.Node ] [yeon-host] stopping ...[2022-01-06T00:15:11,513][INFO ][o.e.n.Node ] [yeon-host] stopped[2022-01-06T00:15:11,514][INFO ][o.e.n.Node ] [yeon-host] closing ...[2022-01-06T00:15:11,587][INFO ][o.e.n.Node ] [yeon-host] closed max file 개수가 부족하여 구동을 못한다는 오류..해결(1) /etc/security/limits.conf 파일 수정. - root로 작업 참고 : ulimit 는 유저(쉘,프로세스)에 대허서 할당할 자원의 한계를 정해서 다중 프로그램/사용자를 기본으로하는 리눅스 시스템에서 과부하를 막아주는 설정 ...yeon soft memlock unlimitedyeon hard memlock unlimitedyeon - nofile 65535... ulimit -a : 유저의 자원 할당량을 조회하는 명령어.[yeon@yeon-host ~]$ ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 15064max locked memory (kbytes, -l) unlimitedmax memory size (kbytes, -m) unlimitedopen files (-n) 65535 # 추가됨pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 4096virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited구동 오류(2)ERROR: [1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch.bootstrap check failure [1] of [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]ERROR: Elasticsearch did not exit normally - check the logs at /home/yeon/dev/elasticsearch/logs/elasticsearch.log[2022-01-06T00:29:48,072][INFO ][o.e.n.Node ] [yeon-host] stopping ...[2022-01-06T00:29:48,122][INFO ][o.e.n.Node ] [yeon-host] stopped[2022-01-06T00:29:48,122][INFO ][o.e.n.Node ] [yeon-host] closing ...[2022-01-06T00:29:48,144][INFO ][o.e.n.Node ] [yeon-host] closed해결(2) /etc/sysctl.conf의 vm.max_map_count 값 수정. - root로 작업...vm.max_map_count=262144" }, { "title": "[Hive-3] Hive 기능 테스트 (derby)", "url": "/posts/ac-hive-test-two/", "categories": "BigData, Hive", "tags": "Hadoop, Hive, HiveQL", "date": "2022-01-04 23:00:00 +0900", "snippet": "hdfs에서 파일 변경 후 hive로 확인 /user/hive/warehouse/person/000000_0_copy_2 파일명 변경 후 정상동작 확인 hdfs 작업 # [1] 변경전 체크[yeon@yeon-host derby]$ hdfs dfs -ls /user/hive/warehouse/person/Found 5 items-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:07 /user/hive/warehouse/person/000000_0-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:07 /user/hive/warehouse/person/000000_0_copy_1-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:08 /user/hive/warehouse/person/000000_0_copy_2 # 변경할 파일-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:08 /user/hive/warehouse/person/000000_0_copy_3-rw-r--r-- 1 yeon supergroup 15 2022-01-04 00:09 /user/hive/warehouse/person/000000_0_copy_4# [2] --- copy_2 파일을 --- copy_9로 변경[yeon@yeon-host derby]$ hdfs dfs -mv /user/hive/warehouse/person/000000_0_copy_2 /user/hive/warehouse/person/000000_0_copy_9# [3] 변경 후 확인[yeon@yeon-host derby]$ hdfs dfs -ls /user/hive/warehouse/person/Found 5 items-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:07 /user/hive/warehouse/person/000000_0-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:07 /user/hive/warehouse/person/000000_0_copy_1-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:08 /user/hive/warehouse/person/000000_0_copy_3-rw-r--r-- 1 yeon supergroup 15 2022-01-04 00:09 /user/hive/warehouse/person/000000_0_copy_4-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:08 /user/hive/warehouse/person/000000_0_copy_9 # 변경된 파일 hive 확인# 변경 전hive&amp;gt; select * from person where input_date = &#39;20200103&#39;;OK20200103 KIM 35Time taken: 3.703 seconds, Fetched: 1 row(s)# 변경 전hive&amp;gt; select * from person where input_date = &#39;20200103&#39;;OK20200103 KIM 35Time taken: 0.195 seconds, Fetched: 1 row(s) 결과 : 정상적으로 조회됨파일을 삭제해보기 hdfs 작업[yeon@yeon-host hive-test]$ hdfs dfs -rm /user/hive/warehouse/person/000000_0_copy_4Deleted /user/hive/warehouse/person/000000_0_copy_4[yeon@yeon-host hive-test]$ hdfs dfs -ls /user/hive/warehouse/personFound 4 items-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:07 /user/hive/warehouse/person/000000_0-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:07 /user/hive/warehouse/person/000000_0_copy_1-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:08 /user/hive/warehouse/person/000000_0_copy_3-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:08 /user/hive/warehouse/person/000000_0_copy_9[yeon@yeon-host hive-test]$[yeon@yeon-host hive-test]$ hdfs dfs -cat /user/hive/warehouse/person/*20200101,KIM,3320200102,BAE,2220200104,LEE,1620200103,KIM,35 hive 확인hive&amp;gt; select count(1) from person;OK5Time taken: 3.797 seconds, Fetched: 1 row(s)hive&amp;gt; select * from person;OK20200101 KIM 3320200102 BAE 2220200104 LEE 1620200103 KIM 35Time taken: 0.305 seconds, Fetched: 4 row(s) 결과 : 파일을 삭제한 후 조회를 하면 4개 행만 조회되지만, count(1)로 할 경우 5로 조회된다. 실데이터는 hdfs의 파일을 조회하지만 count정보는 다른 곳에서 들고오는 듯…?파일을 신규로 생성해보기 hdfs 작업# localPath에 파일 생성[yeon@yeon-host hive-test]$ cat normal20200121,YEON,28# hdfs에 파일 put[yeon@yeon-host hive-test]$ hdfs dfs-put normal /user/hive/warehouse/person hive 확인hive&amp;gt; select count(1) from person;OK5hive&amp;gt; select * from person;OK20200101 KIM 3320200102 BAE 2220200104 LEE 1620200120 YEON 2820200103 KIM 35 결과 : 삭제 테스트와 동일하게 select * 로 조회했을 때는 조회되나 count는 반영되지 않음.형식에 맞지않은 파일을 추가해보기 hdfs 작업# 컬럼 3개중 하나만 값을 넣기[yeon@yeon-host hive-test]$ cat notnormal20200122[yeon@yeon-host hive-test]$ hdfs dfs -put notnormal /user/hive/warehouse/person# 컬럼 3개를 초과하여 입력하기[yeon@yeon-host hive-test]$ cat manydata20200123,YEON,28,1[yeon@yeon-host hive-test]$ hdfs dfs -put manydata /user/hive/warehouse/person hive 확인hive&amp;gt; select * from person;OK20200101 KIM 3320200102 BAE 2220200104 LEE 1620200120 YEON 2820200103 KIM 3520200123 YEON 28 # manydata20200121 YEON 2820200122 NULL NULL # notnormal 결과 : 컬럼개수가 넘치는건 뒤의값을 자르고, 모자른건 null로 조회.기존파일에 레코드를 추가해보기 hdfs 작업# hdfs 파일 조회[yeon@yeon-host hive-test]$ hdfs dfs -cat /user/hive/warehouse/person/000000_0_copy_320200104,LEE,16# local File 생성 (append 용도)[yeon@yeon-host hive-test]$ cat appendTest20220104,KANG,29# local File을 hdfs 파일에 append[yeon@yeon-host hive-test]$ hdfs dfs -appendToFile appendTest /user/hive/warehouse/person/000000_0_copy_3# hdfs 확인[yeon@yeon-host hive-test]$ hdfs dfs -cat /user/hive/warehouse/person/000000_0_copy_320200104,LEE,1620220104,KANG,29 hive 확인hive&amp;gt; select * from person;OK20200101 KIM 3320200102 BAE 2220200104 LEE 1620220104 KANG 29 # append 한 데이터20200120 YEON 2820200103 KIM 3520200123 YEON 2820200121 YEON 2820200122 NULL NULLTime taken: 0.181 seconds, Fetched: 9 row(s) 결과 : 정상적으로 데이터가 조회됨.Hive에 record insert후 확인 hive 작업 및 확인# 신규 데이터 inserthive&amp;gt; insert into person values (&#39;20220104&#39;,&#39;NAM&#39;,&#39;27&#39;);...# 데이터 확인hive&amp;gt; select * from person;OK20200101 KIM 3320200102 BAE 2220220104 NAM 27 # 추가한 데이터20200104 LEE 1620220104 KANG 2920200120 YEON 2820200103 KIM 3520200123 YEON 2820200121 YEON 2820200122 NULL NULLTime taken: 0.18 seconds, Fetched: 10 row(s)# 데이터 건수 확인hive&amp;gt; select count(1) from person;OK6Time taken: 0.23 seconds, Fetched: 1 row(s) 결과 : total count가 1이 증가한 걸 확인..Hive 테이블에 hdfs 경로를 지정하여 load load data inpath &#39;{hadoopFilePath}&#39; into table {table};# 기존 데이터 확인hive&amp;gt; select * from person;OK20200101 KIM 3320200102 BAE 2220220104 NAM 2720200104 LEE 1620220104 KANG 2920200120 YEON 2820200103 KIM 3520200123 YEON 2820200121 YEON 2820200122 NULL NULLTime taken: 0.18 seconds, Fetched: 10 row(s)# count 조회 결과 확인hive&amp;gt; select count(1) from person;OK6Time taken: 0.23 seconds, Fetched: 1 row(s)-&amp;gt; hdfs 상으로는 10개의 row가 들어있으나 count(1) 결과는 6으로 나옴. (hdfs에서 수동으로 넣은 데이터는 조회만 되고 카운팅 되지 않음)---# [20200121,YEON,28] hdfs로 들어가 있는 데이터를 person 테이블에 loadhive&amp;gt; load data inpath &#39;/user/hive/warehouse/person/normal&#39; into table person; /Loading data to table default.personOKTime taken: 0.288 seconds# count 조회 결과 확인hive&amp;gt; select count(1) from person;Query ID = yeon_20220105000220_3ff6dd33-6b87-40d8-bf01-b820414797faTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&amp;lt;number&amp;gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&amp;lt;number&amp;gt;Starting Job = job_1641300868505_0002, Tracking URL = http://yeon-host:8088/proxy/application_1641300868505_0002/Kill Command = /home/yeon/dev/hadoop/bin/mapred job -kill job_1641300868505_0002Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12022-01-05 00:02:31,463 Stage-1 map = 0%, reduce = 0%2022-01-05 00:02:38,896 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.45 sec2022-01-05 00:02:48,324 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 3.06 secMapReduce Total cumulative CPU time: 3 seconds 60 msecEnded Job = job_1641300868505_0002MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 3.06 sec HDFS Read: 13066 HDFS Write: 102 SUCCESSTotal MapReduce CPU Time Spent: 3 seconds 60 msecOK10Time taken: 28.674 seconds, Fetched: 1 row(s)hdfs에 파일을 넣은 후, location 지정하여 hive 테이블 생성 CREATE TABLE {table} ({컬럼 정보}) location &#39;{hadoopFilePath}&#39;; hdfs 작업 # /tmp/hive-test 디렉토리를 생성하여 데이터 insert[yeon@yeon-host hive-test]$ hdfs dfs -cat /tmp/hive-test/*20220104,KANG,2920200121,YEON,28 hive 테이블 생성 후 조회 location 은 ‘hdfs://hostname - 부터 적어도 됨’ # 테이블 생성hive&amp;gt; CREATE TABLE employee (input_date String, name String, age int) location &#39;/tmp/hive-test/&#39;;OKTime taken: 0.295 seconds# 조회hive&amp;gt; select * from employee;OK20220104,KANG,29 NULL NULL20200121,YEON,28 NULL NULLTime taken: 0.236 seconds, Fetched: 2 row(s) hdfs 확인# 따로 employee 디렉토리가 생성되지 않음.[yeon@yeon-host hive-test]$ hdfs dfs -ls /user/hive/warehouse/Found 1 itemsdrwxr-xr-x - yeon supergroup 0 2022-01-05 00:02 /user/hive/warehouse/personhive에서 record insert hive 실행 및 확인# hive inserthive&amp;gt; insert into employee values (&#39;20220105&#39;, &#39;MUN&#39;, 33);Query ID = yeon_20220105001741_bb6928ce-d514-49a6-9d42-40b38f123bedTotal jobs = 3Launching Job 1 out of 3Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&amp;lt;number&amp;gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&amp;lt;number&amp;gt;Starting Job = job_1641300868505_0004, Tracking URL = http://yeon-host:8088/proxy/application_1641300868505_0004/Kill Command = /home/yeon/dev/hadoop/bin/mapred job -kill job_1641300868505_0004Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12022-01-05 00:17:51,730 Stage-1 map = 0%, reduce = 0%2022-01-05 00:18:00,139 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.82 sec2022-01-05 00:18:07,441 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 3.32 secMapReduce Total cumulative CPU time: 3 seconds 320 msecEnded Job = job_1641300868505_0004Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to directory hdfs://yeon-host:9000/tmp/hive-test/.hive-staging_hive_2022-01-05_00-17-41_521_7792296284703375118-1/-ext-10000Loading data to table default.employeeMapReduce Jobs Launched:Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 3.32 sec HDFS Read: 16143 HDFS Write: 287 SUCCESSTotal MapReduce CPU Time Spent: 3 seconds 320 msecOKTime taken: 28.357 seconds# 결과 확인hive&amp;gt; select * from employee;OK20220105 MUN 3320200120,YEON,28 NULL NULL20220104,KANG,29 NULL NULL20200121,YEON,28 NULL NULLTime taken: 0.196 seconds, Fetched: 4 row(s) hdfs 확인[yeon@yeon-host hive-test]$ hdfs dfs -cat /tmp/hive-test/*20220105MUN33 # hive에서 insert한 데이터20200120,YEON,2820220104,KANG,2920200121,YEON,28 hive에서 insert한 경우에는 정상적으로 들어가지고, hdfs에서는 구분자없이 일렬로 들어간것으로 확인.localPath의 파일을 hive에 load hdfs 작업# 위와 비슷하게 구분자없이 일렬로 나열된 데이터 파일 생성.[yeon@yeon-host hive-test]$ cat emptest20220105SUN55 hive 실행 및 확인# hive에 로컬 파일 loadhive&amp;gt; load data local inpath &#39;/home/yeon/dev/test/hive-test/emptest&#39; into table employee;Loading data to table default.employeeOKTime taken: 0.23 seconds# hive 데이터 조회hive&amp;gt; select * from employee;OK20220105 MUN 3320200120,YEON,28 NULL NULL20220104,KANG,29 NULL NULL20220105SUN55 NULL NULL # localfile load 데이터20200121,YEON,28 NULL NULLTime taken: 0.142 seconds, Fetched: 5 row(s) 결과 : 정상적으로 load는 되었지만 구분되지 않고 input_date에 다 들어감. 따로 옵션을 지정해야 할듯.hive partition table load ALTER TABLE employee ADD PARTITION (inputDate=&#39;20170101&#39;) LOCATION &#39;{hadoopFilePath}&#39;; 테스트는 아직 안해봄." }, { "title": "[Hive-2] Hive 실행 및 테스트 (derby)", "url": "/posts/ab-hive-test/", "categories": "BigData, Hive", "tags": "Hadoop, Hive, HiveQL", "date": "2022-01-04 00:00:00 +0900", "snippet": "Hive 설치환경 구성 Tools Version VirtualBox 6.1 CentOS 7 Java Version 1.8 Hadoop 3.3.1 Hive 3.1.2 사용포트 정리 Program Port Number NameNode 9870 DataNode 9864 선행 작업 VirtualBox 설치 VirtualBox에 CentOS 설치 CentOS에 java설치 CentOS에 Hadoop 설치 CentOS에 Hive 설치하이브쉘하이브쉘은 HiveQL 명령어로 하이브와 상호작용할 수 있는 하이브의 기본 도구. (mysql과 매우 유사)실행 및 테스트 hive-site.xml 메타스토어를 등록하지 않고, derby로 테스트. hive 쉘을 사용하여 테스트Hive 스키마 생성 hive-site.xml에 따로 메타스토어를 설정하지 않으면 dbType에 derby를 사용하여 hive를 구동할 수 있다.# derby 테스트용 디렉토리 생성[yeon@yeon-host derby]$ pwd/home/yeon/dev/hive/derby# 스키마 초기화[yeon@yeon-host derby]$ schematool -dbType derby -initSchemaSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/yeon/lib/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/yeon/lib/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Metastore connection URL: jdbc:derby:;databaseName=metastore_db;create=trueMetastore Connection Driver : org.apache.derby.jdbc.EmbeddedDriverMetastore connection User: APPStarting metastore schema initialization to 3.1.0Initialization script hive-schema-3.1.0.derby.sqlInitialization script completedschemaTool completedHive 실행 실행derby를 사용할 경우, 스키마를 초기화한 디렉토리에서 hive를 구동해야 한다.[yeon@yeon-host derby]$ hive 실행 (로그 출력 되도록 옵션 설정)[yeon@yeon-host derby]$ hive --hiveconf hive.root.logger=INFO,console 실행 (Background에서 hiveserver2로 구동)[yeon@yeon-host derby]$ hive --service hiveserver2 &amp;amp;하이브 쉘 명령어 테스트# 데이터베이스 조회hive&amp;gt; show databases;OKdefaultTime taken: 0.035 seconds, Fetched: 1 row(s)# 테이블 조회hive&amp;gt; show tables;OKTime taken: 0.044 seconds# 테이블 생성create table person ( input_date string, name string, age int ) row format delimited fields terminated by &#39;,&#39; lines terminated by &#39;\\n&#39; stored as textfile;# 테이블에 데이터 Insert# 대략 1 row당 30초 정도 소요. (기본 mapreduce 사용)insert into person values (&#39;20200101&#39;, &#39;KIM&#39;, 33);insert into person values (&#39;20200102&#39;, &#39;BAE&#39;, 22);insert into person values (&#39;20200103&#39;, &#39;KIM&#39;, 35);insert into person values (&#39;20200104&#39;, &#39;LEE&#39;, 16);insert into person values (&#39;20200105&#39;, &#39;SO&#39;, 25);...2022-01-04T00:09:21,559 INFO [ccf67a99-62fa-434b-98ab-388e73b9216e main] ql.Driver: OK2022-01-04T00:09:21,559 INFO [ccf67a99-62fa-434b-98ab-388e73b9216e main] ql.Driver: Concurrency mode is disabled, not creating a lock managerTime taken: 32.305 seconds2022-01-04T00:09:21,584 INFO [ccf67a99-62fa-434b-98ab-388e73b9216e main] CliDriver: Time taken: 32.305 seconds # 소요시간# 조회 SQL# 28초 정도 소요 (기본 mapreduce 사용)select name, count(*), max(age), min(age) from person group by name;...2022-01-04T00:11:11,731 INFO [ccf67a99-62fa-434b-98ab-388e73b9216e main] mapred.FileInputFormat: Total input files to process : 1BAE 1 22 22KIM 2 35 33LEE 1 16 16SO 1 25 252022-01-04T00:11:11,754 INFO [ccf67a99-62fa-434b-98ab-388e73b9216e main] exec.ListSinkOperator: RECORDS_OUT_OPERATOR_LIST_SINK_10:4, RECORDS_OUT_INTERMEDIATE:0,Time taken: 28.575 seconds, Fetched: 4 row(s)2022-01-04T00:11:11,757 INFO [ccf67a99-62fa-434b-98ab-388e73b9216e main] CliDriver: Time taken: 28.575 seconds, Fetched: 4 row(s)Yarn Resource Manager 실행 결과 확인 Yarn Resource Manager에서 실행 정보 확인 가능. insert / select 실행 결과 모두 조회 가능.Yarn Resource Manager에서 실행 항목 조회hive로 저장한 데이터 hdfs에서 확인 현재 옵션으로, hive 에서 만든 테이블 정보는 /user/hive/warehouse에 저장된다.# 위 경로 하위에 위에서 만든 테이블명 (person)으로 폴더가 생성됨[yeon@yeon-host conf]$ hdfs dfs -ls /user/hive/warehouseFound 1 itemsdrwxr-xr-x - yeon supergroup 0 2022-01-04 00:09 /user/hive/warehouse/person# ~/person 하위에 위에서 insert한 5개의 파일이 확인[yeon@yeon-host conf]$ hdfs dfs -ls /user/hive/warehouse/personFound 5 items-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:07 /user/hive/warehouse/person/000000_0-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:07 /user/hive/warehouse/person/000000_0_copy_1-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:08 /user/hive/warehouse/person/000000_0_copy_2-rw-r--r-- 1 yeon supergroup 16 2022-01-04 00:08 /user/hive/warehouse/person/000000_0_copy_3-rw-r--r-- 1 yeon supergroup 15 2022-01-04 00:09 /user/hive/warehouse/person/000000_0_copy_4# ~/person 하위에 위에서 insert한 5 record가 csv 형식으로 넣어져있음을 확인.[yeon@yeon-host conf]$ hdfs dfs -cat /user/hive/warehouse/person/*20200101,KIM,3320200102,BAE,2220200103,KIM,3520200104,LEE,1620200105,SO,25# ~/person 하위 파일 한개 확인[yeon@yeon-host conf]$ hdfs dfs -cat /user/hive/warehouse/person/000000_0_copy_220200103,KIM,35hive로 hdfs 저장 위치 조회 hive 에 아래 명령을 내려 hive storage 위치를 찾을수 있다. hive -S -e &quot;DESCRIBE FORMATTED person;&quot; | grep &#39;Location&#39; | awk &#39;{ print $NF }&#39;[yeon@yeon-host derby]$ hive -S -e &quot;DESCRIBE FORMATTED person;&quot; | grep &#39;Location&#39; | awk &#39;{ print $NF }&#39;which: no hbase in (/home/yeon/.local/bin:/home/yeon/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64/bin:/home/yeon/dev/hadoop/sbin:/home/yeon/dev/hadoop/bin:/usr/lib/jvm/jre-1.8.0//bin:/home/yeon/dev/hadoop/bin:/home/yeon/dev/hadoop/sbin:/home/yeon/dev/hive/bin:/home/yeon/.local/bin:/home/yeon/bin)SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/yeon/lib/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/yeon/lib/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Hive Session ID = 9bafda92-c022-4341-8480-f999a314222aHive Session ID = 737bb5c7-d387-4d03-8199-596f40c6424bhdfs://yeon-host:9000/user/hive/warehouse/person # 저장위치 확인derby 사용 + hiveserver2 구동아래와 같이 hive로 process는 띄워져있으나 hive port (default:10000)은 띄워져 있지 않음..mysql이나 mssql로 metastore를 등록해야 하는듯.[yeon@yeon-host ~]$ ps -ef | grep hiveyeon 12468 1529 65 00:22 pts/0 00:00:13 /usr/lib/jvm/jre-1.8.0//bin/java -Dproc_jar -Djava.library.path=/home/yeon/dev/hadoop/lib/native -Dproc_hiveserver2 -Dlog4j.configurationFile=hive-log4j2.properties -Djava.util.logging.config.file=/home/yeon/dev/hive/conf/parquet-logging.properties -Djline.terminal=jline.UnsupportedTerminal -Dyarn.log.dir=/home/yeon/dev/hadoop/logs -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/home/yeon/dev/hadoop -Dyarn.root.logger=INFO,console -Xmx256m -Dhadoop.log.dir=/home/yeon/dev/hadoop/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/yeon/dev/hadoop -Dhadoop.id.str=yeon -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /home/yeon/dev/hive/lib/hive-service-3.1.2.jar org.apache.hive.service.server.HiveServer2yeon 12610 1553 0 00:22 pts/1 00:00:00 grep --color=auto hive[yeon@yeon-host ~]$ netstat -ntlp(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN -tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN -tcp6 0 0 ::1:25 :::* LISTEN -tcp6 0 0 :::13562 :::* LISTEN 2407/javatcp6 0 0 :::46106 :::* LISTEN 2407/javatcp6 0 0 :::8030 :::* LISTEN 2297/javatcp6 0 0 :::8031 :::* LISTEN 2297/javatcp6 0 0 :::8032 :::* LISTEN 2297/javatcp6 0 0 :::8033 :::* LISTEN 2297/javatcp6 0 0 :::8040 :::* LISTEN 2407/javatcp6 0 0 :::9864 :::* LISTEN 1861/javatcp6 0 0 192.168.25.2:9000 :::* LISTEN 1748/javatcp6 0 0 :::8042 :::* LISTEN 2407/javatcp6 0 0 :::9866 :::* LISTEN 1861/javatcp6 0 0 :::9867 :::* LISTEN 1861/javatcp6 0 0 :::9868 :::* LISTEN 2069/javatcp6 0 0 127.0.0.1:41260 :::* LISTEN 1861/javatcp6 0 0 :::9870 :::* LISTEN 1748/javatcp6 0 0 :::22 :::* LISTEN -tcp6 0 0 :::8088 :::* LISTEN 2297/java" }, { "title": "[Hive-1] Hive 설치", "url": "/posts/aa-hive-setting/", "categories": "BigData, Hive", "tags": "Hadoop, Hive", "date": "2022-01-03 22:00:00 +0900", "snippet": "Hive 설치환경 구성 Tools Version VirtualBox 6.1 CentOS 7 Java Version 1.8 Hadoop 3.3.1 Hive 3.1.2 선행 작업 VirtualBox 설치 VirtualBox에 CentOS 설치 CentOS에 java설치 CentOS에 Hadoop 설치설치 hive를 설치설치 위치/home/yeon/devHive 3.1.2 다운로드 및 압축해제cd $LIB_HOMEwget https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gztar -xvzf apache-hive-3.1.2-bin.tar.gzHive 심볼릭 링크 적용cd $DEV_HOMEln -s $LIB_HOME/apache-hive-3.1.2-bin hiveHive 심볼릭 링크 적용까지 작업했을 경우Hive 환경변수 설정~/.bashrc...# Hive Settingexport HIVE_HOME=$DEV_HOME/hiveexport PATH=&quot;$HOME/.local/bin:$HOME/bin:$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin&quot;# bash 파일 변경정보 적용[yeon@yeon-host dev]$ source ~/.bashrcwarehouse 생성따로 설정하지 않으면 기본 경로가 /user/hive/warehouse 이다.hdfs dfs -mkdir /tmphdfs dfs -mkdir -p /user/hive/warehouse# hadoop 클러스터 하나에 여러 사용자가 공유할 수 있으므로 권한을 따로 부여한다.hdfs dfs -chmod g+w /tmphdfs dfs -chmod -R g+w /user# hdfs dfs -chmod -R 777 / # 이건 따로 안줌. (gram에는 주고 samsung에는 안함)참고로 해당 warehouse의 위치를 변경하려면 $HIVE_HOME/conf/hive-site.xml의 아래 속성을 변경하면 됨&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.metastore.warehouse.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;/user/hive/warehouse&amp;lt;/value&amp;gt; &amp;lt;description&amp;gt;location of default database for the warehouse&amp;lt;/description&amp;gt;&amp;lt;/property&amp;gt;warehouse 확인[yeon@yeon-host dev]$ hdfs dfs -ls -R /user/hivedrwxrwxr-x - yeon supergroup 0 2022-01-03 23:34 /user/hive/warehouseguava lib 옮기기 guava : Google의 Java Opensource library. util 관련 라이브러리이다. Hadoop에 있는 guava-27.jar를 Hive/lib 로 옮기기. (버전을 맞추지 않으면 꼬일 수 있으므로..)# Hadoop/lib의 guava-27.jar을 Hive/lib로 이동[yeon@yeon-host dev]$ cp $DEV_HOME/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar $DEV_HOME/hive/lib# Hive/lib에 있던 guava-19.jar 삭제[yeon@yeon-host dev]$ rm $DEV_HOME/hive/lib/guava-19*.jar# guava-*.jar 확인[yeon@yeon-host dev]$ ll $DEV_HOME/hive/lib/guava-*-rw-r--r--. 1 yeon yeon 2747878 1월 3 23:48 /home/yeon/dev/hive/lib/guava-27.0-jre.jar" }, { "title": "[Hadoop-3] Hadoop MapReduce 테스트", "url": "/posts/ac-hadoop-mapreduce-test/", "categories": "BigData, Hadoop", "tags": "Hadoop, MapReduce", "date": "2022-01-03 21:00:00 +0900", "snippet": "Hadoop MapReduce 테스트환경 구성 Tools Version VirtualBox 6.1 CentOS 7 Java Version 1.8 Hadoop 3.3.1 사용포트 정리 Program Port Number NameNode 9870 DataNode 9864 ResourceManager 8088 NodeManager 8042 MapReduce JobHistory Server 19888 선행 작업 VirtualBox 설치 VirtualBox에 CentOS 설치 CentOS에 java설치 CentOS에 hadoop 설치MapReduceMapReduce는 MR이라고도 표현한다.테스트로 WordCount 예제를 실행하여 결과를 확인할 것이다.테스트 데이터 : 공백을 포함한 문자들.결과 : 공백을 구분자로 단어별 개수를 출력.테스트 절차 /tmp/mr/in에 input1.txt, input.csv 두 파일을 업로드하고, mapreduce 실행 후 결과를 /tmp/mr/out 디렉토리에 넣는다. 결과파일을 출력해보고, localFileSystem에 저장한다.MapReduce 샘플 jar하둡 폴더에 MapReduce 샘플 jar가 있으므로, 그걸 사용하여 테스트를 진행할 수 있다.[yeon@yeon-host mapreduce]$ pwd/home/yeon/dev/hadoop/share/hadoop/mapreduce[yeon@yeon-host mapreduce]$ ll | grep hadoop-mapreduce-examples-3.3.1.jar-rw-r--r--. 1 yeon yeon 280989 6월 15 2021 hadoop-mapreduce-examples-3.3.1.jar# 테스트 폴더로 이동[yeon@yeon-host mapreduce]$ cp hadoop-mapreduce-examples-3.3.1.jar $DEV_HOME/test/test-mr테스트 파일 생성[yeon@yeon-host test-mr]$ cat input1.txtaaa bbbccc dddeee[yeon@yeon-host test-mr]$ cat input2.csveeefffgggaaahdfs에 입력데이터를 넣을 디렉토리 생성 /tmp/mr/out 디렉토리(출력용)는 미리 생성해놓으면 mapreduce 실행 시 Exception 이 발생하므로 만들지 않는다.[yeon@yeon-host test-mr]$ hdfs dfs -mkdir -R /tmp/mr/inhdfs에 파일 업로드[yeon@yeon-host test-mr]$ hdfs dfs -put input1.txt /tmp/mr/in[yeon@yeon-host test-mr]$ hdfs dfs -put input2.csv /tmp/mr/inmapreduce 실행 hadoop jar {mapreduce.jar} {작업 클래스명} {hdfs 입력 데이터 디렉토리} {hdfs 출력 데이터 디렉토리}[yeon@yeon-host test-mr]$ hadoop jar hadoop-mapreduce-examples-3.3.1.jar wordcount /tmp/mr/in /tmp/mr/out2022-01-03 21:33:48,664 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:80322022-01-03 21:33:49,210 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/yeon/.staging/job_1641209571726_00012022-01-03 21:33:49,563 INFO input.FileInputFormat: Total input files to process : 2 # 넣은 파일 개수2022-01-03 21:33:50,533 INFO mapreduce.JobSubmitter: number of splits:22022-01-03 21:33:51,243 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1641209571726_00012022-01-03 21:33:51,243 INFO mapreduce.JobSubmitter: Executing with tokens: []2022-01-03 21:33:51,580 INFO conf.Configuration: resource-types.xml not found2022-01-03 21:33:51,580 INFO resource.ResourceUtils: Unable to find &#39;resource-types.xml&#39;.2022-01-03 21:33:52,221 INFO impl.YarnClientImpl: Submitted application application_1641209571726_00012022-01-03 21:33:52,293 INFO mapreduce.Job: The url to track the job: http://yeon-host:8088/proxy/application_1641209571726_0001/2022-01-03 21:33:52,293 INFO mapreduce.Job: Running job: job_1641209571726_00012022-01-03 21:34:02,539 INFO mapreduce.Job: Job job_1641209571726_0001 running in uber mode : false2022-01-03 21:34:02,543 INFO mapreduce.Job: map 0% reduce 0%2022-01-03 21:34:11,771 INFO mapreduce.Job: map 50% reduce 0%2022-01-03 21:34:12,791 INFO mapreduce.Job: map 100% reduce 0%2022-01-03 21:34:18,879 INFO mapreduce.Job: map 100% reduce 100% # mapreduce 진행현황2022-01-03 21:34:20,937 INFO mapreduce.Job: Job job_1641209571726_0001 completed successfully # mapreduce 실행 결과2022-01-03 21:34:21,054 INFO mapreduce.Job: Counters: 55 File System Counters FILE: Number of bytes read=96 FILE: Number of bytes written=820016 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=250 HDFS: Number of bytes written=42 HDFS: Number of read operations=11 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 HDFS: Number of bytes read erasure-coded=0 Job Counters Killed map tasks=1 Launched map tasks=2 Launched reduce tasks=1 Data-local map tasks=2 Total time spent by all maps in occupied slots (ms)=14727 Total time spent by all reduces in occupied slots (ms)=4089 Total time spent by all map tasks (ms)=14727 Total time spent by all reduce tasks (ms)=4089 Total vcore-milliseconds taken by all map tasks=14727 Total vcore-milliseconds taken by all reduce tasks=4089 Total megabyte-milliseconds taken by all map tasks=15080448 Total megabyte-milliseconds taken by all reduce tasks=4187136 Map-Reduce Framework Map input records=7 Map output records=9 Map output bytes=72 Map output materialized bytes=102 Input split bytes=214 Combine input records=9 Combine output records=9 Reduce input groups=7 Reduce shuffle bytes=102 Reduce input records=9 Reduce output records=7 Spilled Records=18 Shuffled Maps =2 Failed Shuffles=0 Merged Map outputs=2 GC time elapsed (ms)=306 CPU time spent (ms)=1300 Physical memory (bytes) snapshot=544653312 Virtual memory (bytes) snapshot=8230359040 Total committed heap usage (bytes)=301146112 Peak Map Physical memory (bytes)=216301568 Peak Map Virtual memory (bytes)=2740568064 Peak Reduce Physical memory (bytes)=113123328 Peak Reduce Virtual memory (bytes)=2749423616 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=36 File Output Format Counters Bytes Written=42실행 결과 확인 Yarn Resource Manager에서 실행 정보 확인 가능.Yarn Resource Manager에서 실행 항목 조회실행 결과 확인 Mapreduce가 성공적으로 끝났을 경우, [출력디렉토리]에 _SUCCESS 파일이 만들어진다. (내용은 없음) part-r-xxxxx 파일에 출력결과가 넣어져있다.[yeon@yeon-host test-mr]$ hdfs dfs -ls /tmp/mr/outFound 2 items-rw-r--r-- 1 yeon supergroup 0 2022-01-03 21:34 /tmp/mr/out/_SUCCESS-rw-r--r-- 1 yeon supergroup 42 2022-01-03 21:34 /tmp/mr/out/part-r-00000[yeon@yeon-host test-mr]$ hdfs dfs -cat /tmp/mr/out/part-r-00000aaa 2bbb 1ccc 1ddd 1eee 2fff 1ggg 1실행 결과를 localFileSystem에 저장# localFileSystem에 출력결과를 저장할 디렉토리 생성(./out)[yeon@yeon-host test-mr]$ mkdir out# hdfs -&amp;gt; localFileSystem 파일 다운로드[yeon@yeon-host test-mr]$ hdfs dfs -get /tmp/mr/out/* ./out# localFileSystem 확인[yeon@yeon-host test-mr]$ ll out합계 4-rw-r--r--. 1 yeon yeon 0 1월 3 21:58 _SUCCESS-rw-r--r--. 1 yeon yeon 42 1월 3 21:58 part-r-00000# localFileSystem 데이터 조회[yeon@yeon-host test-mr]$ cat out/part-r-00000aaa 2bbb 1ccc 1ddd 1eee 2fff 1ggg 1" }, { "title": "[Hadoop-2] Hadoop Command 테스트", "url": "/posts/ab-hadoop-test/", "categories": "BigData, Hadoop", "tags": "Hadoop, Test", "date": "2022-01-03 20:00:00 +0900", "snippet": "Hadoop 테스트환경 구성 Tools Version VirtualBox 6.1 CentOS 7 Java Version 1.8 Hadoop 3.3.1 사용포트 정리 Program Port Number NameNode 9870 DataNode 9864 ResourceManager 8088 NodeManager 8042 MapReduce JobHistory Server 19888 선행 작업 VirtualBox 설치 VirtualBox에 CentOS 설치 CentOS에 java설치 CentOS에 hadoop 설치Command 테스트리눅스 명령어 mkdir, ls, rm 등 실제 명령어와 비슷하다.hdfs dfs를 사용하여, hdfs와 상호작용할 수 있다.hadoop fs와 hdfs dfs 명령어가 있는데, hadoop fs는 hdfs외에 local 파일 시스템도 상호작용 가능하다.mkdir - 디렉토리 생성설치위치는 DEV_HOME으로 지정하여 ~/.bashrc에 변수로 환경변수 등록. hdfs dfs -mkdir [-p] {hdfsPath} -p : 하위 디렉토리까지 전체 생성. [yeon@yeon-host ~]$ hdfs dfs -mkdir -p /tmp/test/ put - 파일 업로드 hdfs dfs -put {localPath} {hdfsPath}# 파일 생성[yeon@yeon-host test]$ cat input1.txtaaaabbbb# Hadoop에 파일 업로드[yeon@yeon-host test]$ hdfs dfs -put input1.txt /tmp/test/ls - 디렉토리 정보 확인 hdfs dfs -ls [-R] {hdfsPath} -R : 하위 디렉토리를 포함한 정보 확인[yeon@yeon-host test]$ hdfs dfs -ls -R /tmp/drwxr-xr-x - yeon supergroup 0 2022-01-03 20:43 /tmp/test-rw-r--r-- 1 yeon supergroup 10 2022-01-03 20:43 /tmp/test/input1.txtcat - 파일 내용 출력 hdfs dfs -cat {hdfsPath}[yeon@yeon-host test]$ hdfs dfs -cat /tmp/test/input1.txtaaaabbbbappendToFile - 파일 내용 추가 hdfs dfs -appendToFile {localPath} {hdfsPath}# 추가할 파일 조회[yeon@yeon-host test]$ cat input3.txtinput3 append# 파일 append[yeon@yeon-host test]$ hdfs dfs -appendToFile input3.txt /tmp/test/input1.txt# append 결과 확인[yeon@yeon-host test]$ hdfs dfs -cat /tmp/test/input1.txtaaaabbbbinput3 appendmv - 파일 / 디렉토리 변경 hdfs dfs -mv {hdfsPath} {hdfsPath}[yeon@yeon-host test]$ hdfs dfs -mv /tmp/test/input1.txt /tmp/test/input2.txt# 변경 내용 확인[yeon@yeon-host test]$ hdfs dfs -ls -R /tmp/drwxr-xr-x - yeon supergroup 0 2022-01-03 21:00 /tmp/test-rw-r--r-- 1 yeon supergroup 10 2022-01-03 20:43 /tmp/test/input2.txtget - hdfs의 파일 local로 가져오기 hdfs dfs -get {hdfsPath} {localPath}[yeon@yeon-host test]$ hdfs dfs -get /tmp/test/input2.txt .# local 파일 확인[yeon@yeon-host test]$ ll합계 8-rw-rw-r--. 1 yeon yeon 10 1월 3 20:42 input1.txt-rw-r--r--. 1 yeon yeon 10 1월 3 21:02 input2.txtchown - hdfs 파일 / 디렉토리 권한 변경 hdfs dfs -chown [-R] {user} {hdfsPath} user에게 hdfs 디렉토리 소유권 변경 [-R] 하위 모든 디렉토리 포함[yeon@yeon-host test]$ hdfs dfs -chown -R yeon2 /tmp/test/input2.txt# 변경 내용 확인[yeon@yeon-host test]$ hdfs dfs -ls /tmp/test/input2.txt-rw-r--r-- 1 yeon2 supergroup 10 2022-01-03 20:43 /tmp/test/input2.txtrm - 파일 / 디렉토리 삭제 hdfs dfs -rm [-r] {user} {hdfsPath} [-r] 하위파일이 있는 디렉토리 삭제.[yeon@yeon-host test]$ hdfs dfs -rm /tmp/test/input2.txtDeleted /tmp/test/input2.txt# 파일이 삭제되었는지 확인[yeon@yeon-host test]$ hdfs dfs -ls /tmp/test/[yeon@yeon-host test]$" }, { "title": "CentOS7 작업 내용", "url": "/posts/aa-centos-setting/", "categories": "Dev, Infomation", "tags": "CentOS7", "date": "2022-01-01 13:00:00 +0900", "snippet": "CentOS7 설치네트워크 도구 설치 해당 툴을 다운받아야 ifconfig 명령어 사용 가능.[yeon@yeon-host dev]$ yum install net-toolsJava 설치 java-1.8.0-openjdk-devel를 설치하면 jps 명령어 사용 가능[yeon@yeon-host dev]$ yum install java-1.8.0-openjdk[yeon@yeon-host dev]$ yum install java-1.8.0-openjdk-devel# Java version 확인[yeon@yeon-host dev]$ java -versionjava version 확인[yeon@yeon-host dev]$ readlink -f /usr/bin/java/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64/jre/bin/java# root 계정으로 /etc/profiles에 환경변수 관련 설정 추가## /etc/profile : 전역 프로파일로 부팅 시점에서 기본적으로 적용되는 환경 설정 파일[root@yeon-host ~]# vi /etc/profile---...## Java Environment Variable SettingJAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64PATH=$PATH:$JAVA_HOME/binCLASSPATH=$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASSPATH---# /etc/profile 변경내용 적용[root@yeon-host ~]# source /etc/profilejava 환경변수 적용 확인jps 명령어 JVM(자바 가상 머신)위에 띄워진 프로세스를 모니터링 할 수 있음.jps 명령어 입력시 출력되는 내용방화벽 설정방화벽 설정 (전체)# 방화벽 켜기[root@yeon-host ~]# systemctl start firewalld# 방화벽 끄기[root@yeon-host ~]# systemctl stop firewalld방화벽 설정 (특정포트)# 특정 포트 방화벽 열기[root@yeon-host ~]# firewall-cmd --permanent --zone=public --add-port=8088/tcpsuccess# 특정포트 방화벽 닫기[root@yeon-host ~]# firewall-cmd --permanent --zone=public --remove-port=8088/tcpsuccess# 방화벽 설정 적용[root@yeon-host ~]# firewall-cmd --reloadsuccess# 방화벽 포트 열렸는지 확인[yeon@yeon-host ~]$ netstat -ntlp# Local Address가 127.0.0.1로 되있으면 Local에서만 접근가능하다는 의미# Local Address가 0.0.0.0 또는 :: 로 되있으면 어디서나 접근가능하다는 의미(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN -tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN -tcp6 0 0 127.0.0.1:35197 :::* LISTEN 7672/javatcp6 0 0 :::8030 :::* LISTEN 8095/javatcp6 0 0 :::8031 :::* LISTEN 8095/javatcp6 0 0 192.168.25.2:9000 :::* LISTEN 7561/javatcp6 0 0 :::41613 :::* LISTEN 8206/javatcp6 0 0 :::9870 :::* LISTEN 7561/javatcp6 0 0 :::22 :::* LISTEN -tcp6 0 0 :::8088 :::* LISTEN 8095/javatcp6 0 0 ::1:25 :::* LISTEN -tcp6 0 0 :::13562 :::* LISTEN 8206/java # 해당 부분 ip와 port로 열렸는지 확인가능. (예시. 8088 포트는 열림)서버시각 동기화# 현재 서버 시각 확인[root@yeon-host ~]# date2022. 01. 02. (일) 02:07:43 KST# ntp 설치[root@yeon-host ~]# yum install ntp# rdate 설치[root@yeon-host ~]# yum install rdate# 원격 서버의 로컬 시각 정보 조회[root@yeon-host ~]# rdate -p time.bora.netrdate: [time.bora.net] Sun Jan 2 02:13:23 2022# 원격 서버 정보를 로컬 시각으로 적용[root@yeon-host ~]# rdate -s time.bora.netntp Network Time Protocol. 네트워크 타임 프로토콜 가변 레이턴시 데이터 네트워크르르 통해 컴퓨터 시스템 간 시간 동기화를 위한 네트워크 프로토콜.rdate Remote Date의 줄임말으로, 로컬 시스템의 시각을 원격 서버의 시각 정보로 맞춰주는 명령어.Bash 파일을 잘못 올렸을 때 잘못입력하여 적용시킬 경우 아무런 명령어도 먹지 않아, 아래처럼 그 경로에 직접 기본값을 넣어 초기화를 해줌.[root@yeon-host ~]# PATH=/usr/local/bin:/bin:/usr/binBash 파일 변경시 source 명령어를 사용하여 적용 ~/.bashrc파일은 로그인할 때 읽는 파일 source 명령어를 사용하지 않으면 적용되지 않으며, 재로그인을 할 경우에는 적용이 된다.[root@yeon-host ~]# vi ~/.bashrc... 작업 ...# source 명령어를 사용하여 변경사항을 적용[root@yeon-host ~]# source ~/.bashrcHost 추가 root 계정 사용/etc/hosts...192.168.25.2 yeon-host" }, { "title": "[Hadoop-1] Hadoop 설치", "url": "/posts/aa-hadoop-setting/", "categories": "BigData, Hadoop", "tags": "Hadoop", "date": "2021-12-27 17:00:00 +0900", "snippet": "Hadoop 설치환경 구성 Tools Version VirtualBox 6.1 CentOS 7 Java Version 1.8 Hadoop 3.3.1 사용포트 정리 Program Port Number NameNode 9870 DataNode 9864 ResourceManager 8088 NodeManager 8042 MapReduce JobHistory Server 19888 선행 작업 VirtualBox 설치 VirtualBox에 CentOS 설치 CentOS에 java설치설치설치 위치/home/yeon/dev설치위치는 DEV_HOME으로 지정하여 ~/.bashrc에 변수로 환경변수 등록.[yeon@yeon-host lib]$ vi ~/.bashrc...# Develop Homeexport DEV_HOME=/home/yeon/dev# Library Homeexport LIB_HOME=/home/yeon/libHadoop 3.3.1 다운로드 및 압축해제cd $LIB_HOMEwget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gztar -xvzf hadoop-3.3.1.tar.gzHadoop 심볼릭 링크 적용cd $DEV_HOMEln -s $LIB_HOME/hadoop-3.3.1 hadoopHadoop 심볼릭 링크 적용까지 작업했을 경우Hadoop 환경변수 설정 /etc/profile에 자바 환경변수 설정을 해놨지만, 따로 설정을 해야함.~/.bashrc...# Java Environment Variable Settingexport JAVA_HOME=/usr/lib/jvm/jre-1.8.0/# Hadoop Settingexport HADOOP_HOME=$DEV_HOME/hadoopexport HADOOP_INSTALL=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/binexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib/native&quot;# bash 파일 변경정보 적용[yeon@yeon-host dev]$ source ~/.bashrccore-site.xml 프로퍼티 설정[yeon@yeon-host dev]$ vi $HADOOP_HOME/etc/hadoop/core-site.xmlcore-site.xml...&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hdfs://yeon-host:9000&amp;lt;/value&amp;gt; # yeon-host 부분은 서버의 호스트명을 넣기 &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hadoop.proxyuser.hadoop.hosts&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hadoop.proxyuser.hadoop.groups&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;hdfs-site.xml 프로퍼티 설정[yeon@yeon-host dev]$ vi $HADOOP_HOME/etc/hadoop/hdfs-site.xmlhdfs-site.xml...&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.name.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;file:///hadoopdata/hdfs/namenode&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.data.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;file:///hadoopdata/hdfs/datanode&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.client.datanode-restart.timeout&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;30&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;mapred-site.xml 프로퍼티 설정[yeon@yeon-host dev]$ vi $HADOOP_HOME/etc/hadoop/mapred-site.xmlmapred-site.xml...&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.app.mapreduce.am.env&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapreduce.map.env&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapreduce.reduce.env&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapreduce.job.user.classpath.first&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;yarn-site.xml 프로퍼티 설정[yeon@yeon-host dev]$ vi $HADOOP_HOME/etc/hadoop/yarn-site.xmlyarn-site.xml...&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.vmem-check-enabled&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;ssh 공개키 생성[yeon@yeon-host dev]$ ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/home/yeon/.ssh/id_rsa):Created directory &#39;/home/yeon/.ssh&#39;.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /home/yeon/.ssh/id_rsa.Your public key has been saved in /home/yeon/.ssh/id_rsa.pub.The key fingerprint is:SHA256:/28Cr8KQOBre+agjAZ6qTYTrfA+i/Qz0i4Oj9iNgs28 yeon@yeon-hostThe key&#39;s randomart image is:+---[RSA 2048]----+| || || ||.. ||+ + . .S ||oO..o o .. ||====.o o .o ||OB*E+o o .o . ||B=@BBo. ....+. |+----[SHA256]-----+[yeon@yeon-host .ssh]$ cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys[yeon@yeon-host .ssh]$ chmod 640 ~/.ssh/authorized_keysHadoopData 생성 root 계정으로 hadoop 데이터 저장 디렉토리를 생성하고, yeon 계정에 모든 권한 부여[root@yeon-host ~]# mkdir -p /hadoopdata/hdfs/namenode[root@yeon-host ~]# mkdir -p /hadoopdata/hdfs/datanode# /hadoopdata 하위에 모든 권한 부여[root@yeon-host ~]# chown -R yeon:yeon /hadoopdataHadoop Namenode 초기화[yeon@yeon-host ~]$ hdfs namenode -formatHadoop 실행[yeon@yeon-host ~]$ start-all.shWARNING: Attempting to start all Apache Hadoop daemons as yeon in 10 seconds.WARNING: This is not a recommended production deployment configuration.WARNING: Use CTRL-C to abort.Starting namenodes on [yeon-host]Starting datanodesStarting secondary namenodes [yeon-host]Starting resourcemanagerStarting nodemanagers# jps 명령어로 아래와 같이 Node/Manager 프로세스가 띄워져있는지 확인[yeon@yeon-host ~]$ jps7672 DataNode7561 NameNode8206 NodeManager8542 Jps7871 SecondaryNameNode8095 ResourceManagerHadoop Namenode 모니터링 페이지http://192.168.25.2:9870/ 참고로 하둡 실행 후, Summary 하위 safemode가 on으로 설정된다. 정상적으로 다 켜질 때까지 기다린 다음 safemode가 off로 전환 된 후에 hive를 구동해야 한다.Hadoop Yarn Resource Manager 모니터링 페이지http://192.168.25.2:8088/참고하둡 실행시 Permission denied 오류 발생 start-all.sh 로 hdfs 구동 시 아래와 같은 에러가 발생함.[yeon@yeon-host ~]$ start-all.shWARNING: Attempting to start all Apache Hadoop daemons as yeon in 10 seconds.WARNING: This is not a recommended production deployment configuration.WARNING: Use CTRL-C to abort.Starting namenodes on [yeon-host]yeon-host: Warning: Permanently added &#39;yeon-host,192.168.25.2&#39; (ECDSA) to the list of known hosts.yeon-host: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).Starting datanodeslocalhost: Warning: Permanently added &#39;localhost&#39; (ECDSA) to the list of known hosts.localhost: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).Starting secondary namenodes [yeon-host]yeon-host: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).Starting resourcemanagerStarting nodemanagerslocalhost: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password). 해결방법ssh 공개키 설정 작업을 잘 했는지 확인 필요.하둡 실행시 User를 못찾는 오류 발생 start-all.sh 로 hdfs 구동 시 아래와 같은 에러가 발생함.ERROR: Attempting to operate on yarn nodemanager as rootERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation. 해결방법$HADOOP_HOME/etc/hadoop/hadoop-env.sh...export HDFS_NAMENODE_USER=&quot;yeon&quot;export HDFS_DATANODE_USER=&quot;yeon&quot;export HDFS_SECONDARYNAMENODE_USER=&quot;yeon&quot;export YARN_RESOURCEMANAGER_USER=&quot;yeon&quot;export YARN_NODEMANAGER_USER=&quot;yeon&quot;" }, { "title": "Post Demo", "url": "/posts/blog-demo/", "categories": "Blogging, Demo", "tags": "demo", "date": "2021-12-27 00:00:00 +0900", "snippet": "This post is to show Markdown syntax rendering on Chirpy, you can also use it as an example of writing. Now, let’s start looking at text and typography.코드블록 종류Images Default (with caption)Full screen width and center alignmentTitlesH1 - headingH2 - headingH3 - headingH4 - headingParagraphI wandered lonely as a cloudThat floats on high o’er vales and hills,When all at once I saw a crowd,A host, of golden daffodils;Beside the lake, beneath the trees,Fluttering and dancing in the breeze.ListsOrdered list Firstly Secondly ThirdlyUnordered list Chapter Section Paragraph Task list TODO Completed Defeat COVID-19 Vaccine production Economic recovery People smile again Description list Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sunBlock Quote This line to shows the Block Quote.Tables Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Linkshttp://127.0.0.1:4000FootnoteClick the hook will locate the footnote1, and here is another footnote2. Shadowshadow effect (visible in light mode) Left aligned Float to left “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Float to right “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dMathematicsThe mathematics powered by MathJax:\\[\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\]When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Inline codeThis is an example of Inline Code.Code blockCommonThis is a common code snippet, without syntax highlight and line number.Specific LanguagesConsole$ env |grep SHELLSHELL=/usr/local/bin/bashPYENV_SHELL=bashShellif [ $? -ne 0 ]; then echo &quot;The command was not successful.&quot;; #do the needful / exitfi;Specific filename@import &quot;colors/light-typography&quot;, &quot;colors/dark-typography&quot;Reverse Footnote The footnote source &amp;#8617; The 2nd footnote source &amp;#8617; " } ]
